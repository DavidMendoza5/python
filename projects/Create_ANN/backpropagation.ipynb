{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation y gradiente descendente\n",
    " * Backpropagation tiene como objetivo minimizar la función de coste ajustando los pesos (w) y sesgos (bias) de la red. El nivel de ajuste está determinado por los gradientes de la función de coste con respecto a esos parámetros. (derivadas)\n",
    "\n",
    "La derivada de una función C mide la sensibilidad al cambio del valor de la función (valor de salida) con respecto a un cambio en su argumento x (valor de entrada). En otras palabras, la derivada nos dice en qué dirección va C.\n",
    "El gradiente muestra cuánto debe cambiar el parámetro x (en dirección positiva o negativa) para minimizar C.\n",
    "Para calcular estos gradientes usamos la tecnica de la Regla de la cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivada de la función de Coste respecto al peso\n",
    "* Se peuede expresar con la regla de la cadena, multiplicando la derivada del Coste respecto a la suma ponderada (z) por la derivada de (z) respecto al valor del peso (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial C}{\\partial w^l_{jk}} =\\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Por\\ definición\\ sabemos\\ que\\ : z^l_j=\\sum_{k=1}^mw^l_{jk}a^{l-1}_{k}+b^l_{j}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Calculando\\ la\\ derivada\\ podemos\\ decir\\ que :\\frac{\\partial z^l_j}{\\partial w^l_{jk}} =a^{l-1}_{k}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Valor\\ final :\\frac{\\partial C}{\\partial w^l_{jk}} =\\frac{\\partial C}{\\partial z^l_j}a^{l-1}_{k}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'\\frac{\\partial C}{\\partial w^l_{jk}} =\\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}}'))\n",
    "display(Math(r'Por\\ definición\\ sabemos\\ que\\ : z^l_j=\\sum_{k=1}^mw^l_{jk}a^{l-1}_{k}+b^l_{j}'))\n",
    "display(Math(r'Calculando\\ la\\ derivada\\ podemos\\ decir\\ que :\\frac{\\partial z^l_j}{\\partial w^l_{jk}} =a^{l-1}_{k}'))\n",
    "display(Math(r'Valor\\ final :\\frac{\\partial C}{\\partial w^l_{jk}} =\\frac{\\partial C}{\\partial z^l_j}a^{l-1}_{k}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivada de la función de Coste respecto al parametro BIAS\n",
    "* Se peuede expresar con la regla de la cadena, multiplicando la derivada del Coste respecto a la suma ponderada (z) por la derivada de (z) respecto al valor del BIAS (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial C}{\\partial b^l_{j}} =\\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_{j}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Calculando\\ la\\ derivada\\ podemos\\ decir\\ que :\\frac{\\partial z^l_j}{\\partial b^l_{j}} =1$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Valor\\ final :\\frac{\\partial C}{\\partial b^l_{j}} =\\frac{\\partial C}{\\partial z^l_j}1$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'\\frac{\\partial C}{\\partial b^l_{j}} =\\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_{j}}'))\n",
    "display(Math(r'Calculando\\ la\\ derivada\\ podemos\\ decir\\ que :\\frac{\\partial z^l_j}{\\partial b^l_{j}} =1'))\n",
    "display(Math(r'Valor\\ final :\\frac{\\partial C}{\\partial b^l_{j}} =\\frac{\\partial C}{\\partial z^l_j}1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La parte común en ambas ecuaciones a menudo se denomina \"gradiente local\" y se expresa de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\delta^l_{j} =\\frac{\\partial C}{\\partial z^l_{j}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Regla\\ de\\ la\\ cadena: \\delta^l_{j} =\\frac{\\partial C}{\\partial a^l_{j}}\\frac{\\partial a^l_{j}}{\\partial z^l_{j}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'\\delta^l_{j} =\\frac{\\partial C}{\\partial z^l_{j}}'))\n",
    "display(Math(r'Regla\\ de\\ la\\ cadena: \\delta^l_{j} =\\frac{\\partial C}{\\partial a^l_{j}}\\frac{\\partial a^l_{j}}{\\partial z^l_{j}}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computar el error de la capa anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\delta^{l-1} =\\delta^lw^l\\frac{\\partial a^{l-1}}{\\partial z^{l-1}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'\\delta^{l-1} =\\delta^lw^l\\frac{\\partial a^{l-1}}{\\partial z^{l-1}}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuando ya tenemos el desarrollo de las variables parciales podemos ajustar los parametros de nuestra red\n",
    "\n",
    "#### Actualizamos el parametro de BIAS usando el Vector Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle b_{l+1} =b_{l} - \\epsilon\\frac{\\partial C}{\\partial b}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\epsilon = El\\ ratio\\ de\\ aprendizaje\\ (Cuanto\\ nos\\ deplazamos\\ en\\ nuestra\\ función\\ de\\ coste\\ en\\ cada\\ iteración)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'b_{l+1} =b_{l} - \\epsilon\\frac{\\partial C}{\\partial b}'))\n",
    "display(Math(r'\\epsilon = El\\ ratio\\ de\\ aprendizaje\\ (Cuanto\\ nos\\ deplazamos\\ en\\ nuestra\\ función\\ de\\ coste\\ en\\ cada\\ iteración)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actualizamos el parametro de pesos usando el vector Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle w_{l+1} =w_{l} - \\epsilon\\frac{\\partial C}{\\partial w}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\epsilon = El\\ ratio\\ de\\ aprendizaje\\ (Cuanto\\ nos\\ deplazamos\\ en\\ nuestra\\ función\\ de\\ coste\\ en\\ cada\\ iteración)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'w_{l+1} =w_{l} - \\epsilon\\frac{\\partial C}{\\partial w}'))\n",
    "display(Math(r'\\epsilon = El\\ ratio\\ de\\ aprendizaje\\ (Cuanto\\ nos\\ deplazamos\\ en\\ nuestra\\ función\\ de\\ coste\\ en\\ cada\\ iteración)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos este proceso interativamente hasta que conseguimos minimizar el error en nuestra función de coste.\n",
    "***Algoritmo del Gradiente Descendente***\n",
    "\n",
    "![](images/gradientdescent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
